{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Code\n",
    "\n",
    "With both X and Y trajectories right now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from file\n"
     ]
    }
   ],
   "source": [
    "with open('prepare_trajectories.pkl', 'rb') as handle:\n",
    "    pkl_obj = pickle.load(handle)\n",
    "    container_t_np = pkl_obj['t']\n",
    "    container_x_np = pkl_obj['x']\n",
    "    container_y_np = pkl_obj['y']\n",
    "    print('Data loaded from file')\n",
    "\n",
    "num_of_trajs = container_y_np.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the prior parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior for longitudinal trajectories loaded\n",
      "Prior for lateral trajectories loaded\n"
     ]
    }
   ],
   "source": [
    "with open('y.pkl', 'rb') as handle:\n",
    "    pkl_obj = pickle.load(handle)\n",
    "    mean_y = pkl_obj['mean_curve']\n",
    "    gm_y_mean = pkl_obj['mu']\n",
    "    gm_y_cov = pkl_obj['cov']\n",
    "    y_basis = pkl_obj['basis']\n",
    "print('Prior for longitudinal trajectories loaded')\n",
    "\n",
    "with open('x.pkl', 'rb') as handle:\n",
    "    pkl_obj = pickle.load(handle)\n",
    "    mean_x = pkl_obj['mean_curve']\n",
    "    gm_x_mean = pkl_obj['mu']\n",
    "    gm_x_cov = pkl_obj['cov']\n",
    "    x_basis = pkl_obj['basis']\n",
    "print('Prior for lateral trajectories loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm \n",
    "def likelihood(z, theta, t, x_or_y = 'y'):\n",
    "    friendly_basis = y_basis\n",
    "    if x_or_y == 'x':\n",
    "        friendly_basis = x_basis\n",
    "    reconstructed_trajectory = theta.T @ friendly_basis \n",
    "    #print('Mean: ', reconstructed_trajectory[t])\n",
    "    #print('Test: ', z)\n",
    "    #raise Exception(\"LALALA\")\n",
    "    return norm.pdf(z, loc=reconstructed_trajectory[t], scale=3.0) # scale is the std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low variance resampling\n",
    "def lvr(particles, weights): # requires normalized weights first\n",
    "    # return: new particles with equal weights\n",
    "    M = particles.shape[0] # number of particles\n",
    "    new_particles = np.zeros_like(particles)\n",
    "    # new_particles_weight = np.ones((M,)) / M \n",
    "    # W = np.sum(weights)\n",
    "    r = np.random.rand() / M # get a random number between 0 and 1/M\n",
    "    c = weights[0]\n",
    "    i = 0\n",
    "    for m in range(M):\n",
    "        U = r + m/M\n",
    "        while U>c:\n",
    "            i+=1\n",
    "            c+=weights[i]\n",
    "        new_particles[m] = particles[i]\n",
    "    return new_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Inferential Spherical Coding for Y component (RISC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the whole idea is similar to delta compression: as the estimation of the representation vector $a$ gets better with observations arriving, we will subsequently broadcast the change to other vehicles, which is the delta compared to the last estimate. All MAP estimates start from the prior's mean $\\overline{a}$, then gradually moves to the corresponding generating representation vector $a^*$.\n",
    "\n",
    "Encoding the radial component should be simple: we send a 1 if we want to move to the next annulus of lattice in the outward radial direction. We send a 0 if we want to move in inward radial direction. Overall, the less common the trajectory is, the more bits we have to send (in multiple attempts) to other vehicles. Vice-versa, because most trajectories can be represented close to the mean, we expect to send little bits to determine the \"correct\" annulus $a^*$ is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\nxf67027\\\\Documents\\\\anomaly-detection-ngsim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm \n",
    "from coding import huffman\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_minus(a,b):\n",
    "    d = np.mod(a - b, np.pi * 2)\n",
    "    return (d > np.pi) * (np.pi * 2 - d) + (d <= np.pi) * d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test against multiple trajectories, each trajectory will have 2 configurations: a uniform distribution and a spherical code distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for one configuration, we have to copy it and run two times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "multivariate_normal.rvs(mean = gm_y_mean, cov = gm_y_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\12/ipykernel_17504/3375404303.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6969\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_trajectory_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# To store the test results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_bits_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_trajectory_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m140\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# set 140 to y_test below too\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrecons_err_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_trajectory_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m140\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_vec' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(6969)\n",
    "test_trajectory_index = np.random.randint(container_y_np.shape[0], size=10)\n",
    "# To store the test results\n",
    "test_bits_log = np.zeros((test_trajectory_index.shape[0], 140)) # set 140 to y_test below too\n",
    "recons_err_log = np.zeros((test_trajectory_index.shape[0], 140))\n",
    "test_configuration = 'spherical' # spherical or uniform\n",
    "\n",
    "for test_no, y_test_id in enumerate(test_trajectory_index.tolist()):\n",
    "    # generate the test trajectory\n",
    "    y_test = container_y_np[y_test_id,:140]\n",
    "    print('Test number {}/{} (trajectory {}) '.format(test_no + 1, test_trajectory_index.shape[0], y_test_id))\n",
    "\n",
    "    ###\n",
    "\n",
    "    n_particles = 5000 \n",
    "    map_particle_available = False\n",
    "\n",
    "    # Step 1: generate a bunch of particles from the prior distribution\n",
    "    particles = kde.sample(n_particles, random_state=6969)\n",
    "    weights = np.ones((n_particles,)) / n_particles\n",
    "    map_components_log = np.zeros((y_test.shape[0], particles.shape[1]))\n",
    "    weights_variance_log = np.zeros((y_test.shape[0],))\n",
    "    measurement_likelihood_log = np.zeros((y_test.shape[0],))\n",
    "    error_log = np.zeros((y_test.shape[0],)) # the error we observe (due to MAPE has changed)\n",
    "    reconstruction_error_log = np.zeros_like(error_log) # the error our clients will observe\n",
    "    broadcast_log = np.zeros((y_test.shape[0],))\n",
    "    bits_log = []\n",
    "    last_transmission_at_time = 0\n",
    "    # Some debuggin variables\n",
    "    beta_sd_history = np.zeros((y_test.shape[0], 4))\n",
    "    betac_history = np.zeros((y_test.shape[0], 4))\n",
    "\n",
    "\n",
    "    lattice_size = 1/np.sqrt(np.shape(particles[0])[0]) # 1/sqrt(5) in case of 5 basis components\n",
    "\n",
    "    for t in range(y_test.shape[0]):\n",
    "        # When measurement arrives, calculate the importance weights first\n",
    "        weights_normalization_coeff = 0\n",
    "        for j in range(weights.shape[0]):\n",
    "            measurement_likelihood = likelihood(y_test[t], particles[j,:], t)\n",
    "            weights[j] = measurement_likelihood\n",
    "            weights_normalization_coeff += weights[j]\n",
    "            measurement_likelihood_log[t] += measurement_likelihood\n",
    "\n",
    "        # print(t, measurement_likelihood)\n",
    "\n",
    "        # Normalization all the weights\n",
    "        for j in range(weights.shape[0]):\n",
    "            weights[j] = weights[j] / weights_normalization_coeff\n",
    "        \n",
    "        if not map_particle_available: # first assignment of map_particle_value\n",
    "            # Get the index of the particle with largest weight, that will be our maximum-a-posteriori estimate \n",
    "            map_particle_index = np.argmax(weights)\n",
    "            map_particle_value = particles[map_particle_index]\n",
    "            mean_particle = map_particle_value.copy()\n",
    "            map_components_log[t,:] = map_particle_value\n",
    "            map_particle_available = True\n",
    "\n",
    "            # Intialization of variables at the beginning of the inference process\n",
    "            alpha = np.zeros_like(mean_particle)\n",
    "            # alphac = np.zeros_like(alpha)\n",
    "            beta = np.zeros((np.shape(mean_particle)[0]-1,))\n",
    "            beta_mean = beta.copy() # this is the smoothed out mean of the beta vector using beta in the past for which we will use to build the Huffman's probability table \n",
    "            # beta_sd = np.ones_like(beta_mean) * np.pi / 4 # this is the variance vector to accompany with the mean vector above\n",
    "            beta_sd = np.ones_like(beta_mean) * np.pi * 4 # starting from a more uniform distribution\n",
    "            betaq = np.zeros_like(beta)\n",
    "            rq = 0 \n",
    "            phi = np.zeros_like(beta)\n",
    "            phiq = np.zeros_like(beta)\n",
    "            map_particle_value_c = map_particle_value.copy()\n",
    "            \n",
    "            \n",
    "        error_log[t] = np.abs(((map_particle_value_c) @ friendly_basis)[t] - y_test[t])\n",
    "        \n",
    "        map_particle_index = np.argmax(weights)\n",
    "        map_particle_value_new = particles[map_particle_index]\n",
    "        \n",
    "        if error_log[t] > 3.0: # if the prediction from MAP deviates up to 2ft from the actual observations\n",
    "            # we will have to broadcast an update so that our fellows can adjust accordingly\n",
    "            alpha_new = map_particle_value_new - mean_particle\n",
    "            delta = alpha_new - alpha \n",
    "            \n",
    "            # calculating radial component\n",
    "            delta_r = np.linalg.norm(delta) # change in radius\n",
    "            delta_rq = np.floor(delta_r / lattice_size) # quantized change in radius\n",
    "            delta_rc = delta_rq * lattice_size # reconstructed change in radius \n",
    "            r_bits = delta_rq\n",
    "            if delta_rc == 0: \n",
    "                delta_rc = 1\n",
    "                r_entropy = 1\n",
    "            else:\n",
    "                r_entropy = np.log2(delta_rq) \n",
    "\n",
    "            last_transmission_at_time = t\n",
    "\n",
    "\n",
    "            # calculating the spherical coordinates (i.e., the beta vector)\n",
    "            beta_new = np.zeros((np.shape(mean_particle)[0]-1,))\n",
    "            for dimension in range(np.shape(mean_particle)[0]-1):\n",
    "                # formula for spherical coordinates (first 3 components)\n",
    "                beta_new[dimension] = np.arccos(delta[dimension]/np.linalg.norm(delta[dimension:]))\n",
    "            # spherical coordinate for the final component\n",
    "            if delta[-1] >= 0:\n",
    "                beta_new[-1] = np.arccos(delta[-2]/np.linalg.norm(delta[-2:]))\n",
    "            else:\n",
    "                beta_new[-1] = 2 * np.pi - np.arccos(delta[-2]/np.linalg.norm(delta[-2:]))\n",
    "\n",
    "            # quantizing the spherical coordinates\n",
    "            number_of_lattices = np.floor(2 * np.pi * delta_rc / lattice_size) + 1\n",
    "            phi_new = np.zeros_like(beta_new)\n",
    "            betac_new = np.zeros_like(phi_new)\n",
    "            for dimension in range(np.shape(mean_particle)[0]-1):\n",
    "                phi_new[dimension] = np.floor(delta_rc * beta_new[dimension] / lattice_size) # to be encoded with Huffman\n",
    "                betac_new[dimension] = phi_new[dimension] * lattice_size / delta_rc # to be used with rc to reconstruct the alphac vector\n",
    "\n",
    "            # reconstruct the alpha vector (the thing our clients will get)\n",
    "            deltac = np.zeros_like(alpha_new)\n",
    "            for i in range(np.shape(mean_particle)[0]-1):\n",
    "                v = delta_rc\n",
    "                for k in range(i):\n",
    "                    v = v * np.sin(betac_new[k])\n",
    "                v = v * np.cos(betac_new[i])  \n",
    "                deltac[i] = v \n",
    "            deltac[-1] = deltac[-2] / np.cos(betac_new[-1]) * np.sin(betac_new[-1])\n",
    "\n",
    "            # Reconstruction of the MAP particle from what the client receives\n",
    "            alphac_new = alpha + deltac # this is what our client will get\n",
    "            map_particle_value_c = alphac_new + mean_particle # this is the particle our client will use to extrapolate the trajectory \n",
    "\n",
    "            # print('Alpha distance: ', np.linalg.norm(alphac_new - alpha_new))\n",
    "\n",
    "            # encode the phi_new vector \n",
    "            phi_entropy = 0\n",
    "            phi_bits = np.zeros_like(phi_new)\n",
    "            phi_prob = np.zeros_like(phi_bits)\n",
    "            for dimension in range(np.shape(mean_particle)[0]-1):\n",
    "                # first, we need to create a probability table to derive Huffman's code\n",
    "                if number_of_lattices < 2: # ensure there will be a minimum of 2 lattices to encode\n",
    "                    p_lattice = np.ones((2,))\n",
    "                else:\n",
    "                    p_lattice = np.zeros((int(number_of_lattices),))\n",
    "                for i in range(int(number_of_lattices)):\n",
    "                    if test_configuration == 'spherical':\n",
    "                        p_lattice[i] = norm.pdf(angular_minus(2 * np.pi / number_of_lattices * i, beta_mean[dimension]), 0, beta_sd[dimension])\n",
    "                    elif test_configuration == 'uniform':\n",
    "                        p_lattice[i] = 1 # uniform\n",
    "                    else:\n",
    "                        raise Exception('test_configuration is not set')\n",
    "                if np.linalg.norm(p_lattice) < 1e-2:\n",
    "                    p_lattice = np.ones_like(p_lattice) # preventing p_lattice from being too small\n",
    "                p_lattice = p_lattice / np.linalg.norm(p_lattice)\n",
    "                # generate the Huffman code (see Useful Snippet 1 below)\n",
    "                p_lattice_sort_arg = np.argsort(p_lattice)[::-1]\n",
    "                p_lattice_sorted = p_lattice[p_lattice_sort_arg]\n",
    "                p_lattice_sort_map = np.argsort(p_lattice_sort_arg)\n",
    "                # generate \n",
    "                code = huffman.HuffmanCode(p_lattice_sorted.tolist())\n",
    "                spherical_code = code.compute_code()\n",
    "\n",
    "                for index, s in enumerate(spherical_code):\n",
    "                    if s == '':\n",
    "                        spherical_code[index] = '0'\n",
    "\n",
    "                phi_prob[dimension] = p_lattice_sorted[p_lattice_sort_map[int(phi_new[dimension])]]\n",
    "                phi_bits[dimension] = int(spherical_code[p_lattice_sort_map[int(phi_new[dimension])]], 2) # convert a string of binary to int\n",
    "                if phi_bits[dimension]<1e-2:\n",
    "                    phi_bits[dimension] += 0\n",
    "                else:\n",
    "                    phi_entropy += np.log2(phi_bits[dimension])\n",
    "\n",
    "            # Update beta_mean and beta_sd\n",
    "            angular_dist = angular_minus(betac_new, beta_mean)\n",
    "            beta_mean = 0.3 * beta_mean + 0.7 * betac_new \n",
    "            beta_sd = 0.7 * (np.abs(angular_dist) * 1.96) + 0.3 * beta_sd\n",
    "\n",
    "            # print('Radial/angular delta vector: {} / {}'.format(r_bits, phi_bits))\n",
    "            if False: #silencing the logging process\n",
    "                print('Codeword proba: ', phi_prob)\n",
    "                print('Ang dist btwn trans (deg): {}'.format(angular_dist / np.pi * 180))\n",
    "                print('Rad bits / Ang bits: {:.2f}/{:.2f}'.format(r_entropy, phi_entropy))\n",
    "                #print('W/O bef/aft cmpr: {:.0f} -> {:.0f} ({:.2f}%) bits'.format(5 * 32, r_entropy + phi_entropy, (r_entropy + phi_entropy) / (5 * 64) * 100))\n",
    "                print('--- END OF TRANSMISSION ATTEMPT ---')\n",
    "            \n",
    "\n",
    "            # Assign new values to current values\n",
    "            # map_particle_value = map_particle_value_new_c.copy() # should reconstruct alpha from quantized parameters and place it here\n",
    "            alpha = alphac_new.copy()\n",
    "            beta = betac_new.copy() # reconstructed from quantization \n",
    "            phi = phi_new.copy()  # reconstructed from quantization too\n",
    "            map_components_log[t,:] = map_particle_value\n",
    "            broadcast_log[t] = 1\n",
    "            bits_log.append(r_entropy + phi_entropy)\n",
    "            # logging for debugging\n",
    "            beta_sd_history[t,:] = beta_sd\n",
    "            betac_history[t,:] = betac_new\n",
    "\n",
    "            test_bits_log[test_no, t] = r_entropy + phi_entropy # for batch testing log\n",
    "\n",
    "            # sys.exit()\n",
    "\n",
    "        reconstruction_error_log[t] = np.abs(((map_particle_value_c) @ friendly_basis)[t] - y_test[t])\n",
    "        recons_err_log[test_no, t] = reconstruction_error_log[t] \n",
    "        \n",
    "        if reconstruction_error_log[t] > 2.00:\n",
    "            # raise Exception(\"While this error is so high?\")\n",
    "            pass \n",
    "        \n",
    "        # Resample the particles according to the particle weights\n",
    "        if measurement_likelihood_log[t] < 100:\n",
    "            particles = lvr(particles, weights)\n",
    "            weights = np.ones((n_particles,)) / n_particles\n",
    "\n",
    "        weights_variance_log[t] = np.var(weights)\n",
    "\n",
    "\n",
    "    ###\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64b90a637096cb0c3a78b87e1faf5cd67d51ac54caf03738def4aea1bc0fda76"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('liberty')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
